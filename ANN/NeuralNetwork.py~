"""
1 - pour un apprentissage non superviser il faut penser distribution de probabilite p(x),
2 - Donnee categorique: ex A = {femme, homme}, ou B = {oui, non, peut-etre},
converti donnee categorique vers donnee numerique (pretraitement): A = {1,0} ou {0, 1} et ,
B = {1,0,0} ou {0,1,0} ou {0,0,1},
3 - valeur manquante (il est possible qu'une valeur manque), 
       X = {1,2, ?, 5}=>pretraitement X = {1,2, '0,1', 5},
    si X = {1,2, 3, 5}=>pretraitement X = {1,2, '3,0', 5},
4 - Il est preferable de normaliser les donnee reelle pour le traitement, afin qu'elle prennemt de valeur 
    proche de 0,
    valeur trop eleve pourraient creer des instabilite numerique
5 - comment determiner si un Algorithm C est meilleur qu'un algorithm D ?
     - on peut regarder l'erreur de test, i.e, mesure de leur performence generaliser
     - Intervlle de confiance, on pourrait etre capable de dire que C est meilleur ou performant que D
     cours apprentissage automatique, 1.10 et 1.11
"""
import matplotlib.pyplot as plt
import numpy as np
import os
import csv
import random
    
class NeuralNet(object):
    # ------------------------------------------------------------------------
    # ------------------- Preparing the files and Data------------------------
    # ------------------------------------------------------------------------        
    # find the repertory and path of the Data files
    def file_path(self):
        # find the file contening the Data that we need
        real_path = os.path.abspath('')
        sub_path = real_path.split('/')
        sub_path.pop() # remove the end of the list
        self.files_path = '/'.join(sub_path)+'/files/'
                
        return self.files_path 
        
    # Change the categorical Data to numerical Data    
    def pre_selection_phase(self, Data_set):
        categoryOutput = []
        for i in range(len(Data_set)):            
            targetOutput = Data_set[i][-1] # get the categorical output
            if targetOutput not in categoryOutput:
                categoryOutput.append(targetOutput)
                # CategoryData = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']
        for line in range(len(Data_set)):                
            for cat in range(len(categoryOutput)):
                if categoryOutput[cat] in Data_set[line]:
                    # The category value is converted to numercal value in CategVect
                    CategVect = ['0']*len(categoryOutput) 
                    # Get the position of the categorical output
                    Index = categoryOutput.index(categoryOutput[cat])
                    # Set the numerical output value to 1
                    CategVect[Index] = '1'                    
                    Data_set[line].pop() # Remove the categorical targeted value
                    # Remplace the categorical to new numerical output
                    Data_set[line].extend(CategVect)                    
                    break
                
        return Data_set
        
    # features extraction function
    def feature_extraction(self, features):           
        Data = []
        labels_set = []         
        files = features["fileName"]+features["fileExtension"]       
        csvfile = open(self.file_path() + files, "r")
        lines = csv.reader(csvfile)
        Data_set = list(lines)
        # Find the Data training lengh
        TrainingDataLengh = int(len(Data_set)*0.6)
        ValidationDataLengh = int(len(Data_set)*0.6)
        TestingDataLengh = int(len(Data_set)*0.6)       
        
        if (features["Catg"]): # if we have categorical data
            # This function convert the categorical data to numerical data
            Data_set = self.pre_selection_phase(Data_set)
            
            # convert the Data to two dimensionals
           # for i in range(len(Data_set)):
                #del Data_set[i][2] 
                #del Data_set[i][2]
            #print '2 - ', Data_set[0]
            
        ''' dataset[0] = ['5.1', '3.5', '1.4', '0.2', 'Iris-setosa']
        les chiffres entre les '' sont consideres comme des chaines de 
        carateres donc il faut les Transformer en nombres ''' 
        
        for row_i in range(len(Data_set)):
            for i in range(len(Data_set[row_i])):
                Data_set[row_i][i] = float(Data_set[row_i][i])                
            Data.append(Data_set[row_i][0:features["inputLen"]])
            labels_set.append(Data_set[row_i][features["inputLen"]:])  
        TrainData = Data[0:TrainingDataLengh]        
        target_labels = labels_set[0:TrainingDataLengh]
        print 'Data',Data_set[0] 
        ValidationData = Data[TrainingDataLengh:ValidationDataLengh]
        validation_labels = labels_set[TrainingDataLengh:ValidationDataLengh]  
        
        Testing_data = Data[TestingDataLengh:]
        Testing_labels = labels_set[TestingDataLengh:]
        # close the csvfile file
        csvfile.close()  
        # preparing the data
        Data = {
            "training": TrainData,
            "targetLabels": target_labels,
            "validation": ValidationData,
            "labelsValidation": validation_labels,
            "testing": Testing_data,
            "testingLabels": Testing_labels
        }
        return Data
    # -----------------------------------------------------------------------
    # ---------------- Training the artificial neural Network----------------
    # ------------------------------------------------------------------------    
    # training phase for the neural network
    def training_phase(self, TrainingData):
        # initialization of the number of layers, matrices and cost function
        self.layers_dictionnaire(TrainingData)
        # initialization of the aprameters of the network
        self.matrix_initialization()
        T = 553
        for n in range(T):
            Ep = [] # initialization of Ei
            for i in range(len(TrainingData["inputData"])):
                X = TrainingData["inputData"][i]
                Y = TrainingData["target_labels"][i]                
                self.forward_propagation(X) # forward propagation
                # cost function
                Ep.append(self.squared_error(Y, self.matrices["forward"][-1]))                
                self.backward_propagation(Y) # backward propagation                
                self.adjust_weights(TrainingData) # ajust the weight wij    
                #print "target", Y, '-', 'output', self.matrices["forward"][-1]  
            self.costFunctionAverage(Ep) 
            #print 'n is', n, '-', i
            #print "target", Y, '-', 'output', self.matrices["forward"][-1]
        plt.plot(self.cost_average["costAverage"])
        #print 'n is', n
      # Compute the prediction error 
    def squared_error(self, desiredOutput, estimatedOutput): 
        Ei = []# squared error list
        for i in range(len(desiredOutput)):
            Ei.append((desiredOutput[i]-estimatedOutput[i])**2)        
        return (0.5*sum(Ei))
    
    # get the error   
    def costFunctionAverage(self, cost):
        #print 'cost', cost
        self.cost_average["costAverage"].append(np.mean(cost))
        
    # testing phase for the neural network       
    def testing_phase(self, TestingData):
        for i in range(len(TestingData)):
            inputValue = TestingData[i]        
            self.forward_propagation(inputValue)
            
        return self.matrices["forward"][:][-1][0]
        
    # dictionnaire for the neural network
    def layers_dictionnaire(self, FromTrainingData):
        # define HyperParameters
        inputData = FromTrainingData["inputData"][0]
        outputData = FromTrainingData["target_labels"][0]
        #hiddenNodes = np.sqrt(len(inputData)-len(outputData))+ random.randint(2,10)      
        self.layers = {
            # number of input (features) of the neural 
            "input": inputData,
            # number of the Hidden layers of the neural 
            "hidden": len(inputData)*len(outputData),
            # number of the neural in the Output layer
            "output": outputData                       
        } 
        # Cost function of the Neural Network
        self.squaredError = {
            "Ei": []
        }
        # Root Squart
        self.cost_average = {
           "costAverage":[]
        }
        # Matrix Wij
        self.matrices = {
            "weights": [],
            "forward": [],
            "backward": []                            
        }
                        
    # function to initialize the parameters of the neural network
    def matrix_initialization(self):     
        # self.m_error_sqrt = []        
        NetworkLayers = [] # preparing the big matrix contening wij 
        NetworkLayers.append(len(self.layers["input"])) 
        NetworkLayers.append(self.layers["hidden"])   
        NetworkLayers.append(len(self.layers["output"]))        
        for layer in range(len(NetworkLayers)-1):
            row = NetworkLayers[layer] + 1 # rows of the matrix wij and plus 1 to add the bias 
            col = NetworkLayers[layer+1] # colunm of the matrix wij
            #weight = np.full((row, col), 0.0001*np.random.random((row, col)), dtype=float)
            #self.matrices["weights"].append(weight) 
            
            weight = np.full(shape=(row, col), fill_value = 0.0)
            #initialization of the weight with random value
            for i in range(len(weight)):
                for j in range(len(weight[0,:])):                    
                    weight[i][j] = random.uniform(-0.1, 0.1)
            self.matrices["weights"].append(weight) 
           
    # forward propagation function                       
    def forward_propagation(self,inputData):
        bias = 1
        self.matrices["forward"] = [] 
        # Convert the Data list to numpy matrix             
        DataMatrix = np.asarray(inputData)
        X = np.append(DataMatrix, bias) # add the bias in the input
        self.matrices["forward"].append(X) # forward matrix
        Wij = self.matrices["weights"][0]        
        Zj = np.dot(X,Wij) # Z = X*W 
        # we pass Zj through the activation function               
        sigmoid = self.sigmoid(Zj) 
        sigmAddBias = np.append(sigmoid, bias) # [sigmoid matrix  bias]       
        self.matrices["forward"].append(sigmAddBias)
        for matrix in range(1,len(self.matrices["weights"])):
            X = self.matrices["forward"][-1]
            Zi = np.dot(X, self.matrices["weights"][matrix])
            sigmoid = self.sigmoid(Zi)
            sigmAddBias = np.append(sigmoid, bias)
            if (matrix==len(self.matrices["weights"])-1):
                sigmAddBias = sigmoid
            self.matrices["forward"].append(sigmAddBias)
        
    # Sigmoid function       
    def sigmoid(self, Z):
        # Apply sigmoid activation function
        return 1/(1 + np.exp(-Z))
    # Derivative of the sigmoid function    
    def sigmoidPrime(self, Z):
         # Derivative of the sigmoid function
         return np.exp(-Z)/((1 + np.exp(-Z))**2)

    # Propagate deltas backward from outputs layers to inputs layers 
    def backward_propagation(self, desiredOutput):
        self.matrices["backward"] = []        
        desired_output = np.asarray(desiredOutput, dtype=float)
        estimated_output = self.matrices["forward"][-1]
        # how much did we miss?
        error = desired_output-estimated_output
        nb_layer = len(self.matrices["weights"])-1
        for layer in range(nb_layer, -1, -1):
            #Wi = self.matrices["weights"][layer]
            #forward = self.matrices["forward"][layer]
            # through sigmoid prime function
            K = self.sigmoidPrime(self.matrices["forward"][layer+1])
            if (layer==nb_layer):
                delta_Oi = []
                # c'est bien de faire autrement
                for i in range(len(error)):
                    delta_Oi.append(K[i]*error[i])
                self.matrices["backward"].append(np.asarray(delta_Oi))
                
            else:
                Wj = self.matrices["weights"][layer+1]
                # Wj matrix without the bias weight
                WjPrime = np.transpose(Wj[0:-1][:])
                detlaTimesWj = np.dot(self.matrices["backward"][-1],WjPrime)
                delta_Oj = []
                # ici aussi ca serai bien de faire autrement
                for j in range(len(detlaTimesWj)):
                    delta_Oj.append(K[j]*detlaTimesWj[j])
                self.matrices["backward"].append(np.asarray(delta_Oj))
               
    # Update every weight in network using deltas 
    def adjust_weights(self, ForTraining):
        L = len(self.matrices["backward"])-1
        alpha = ForTraining["alpha"]
        eta = ForTraining["eta"]
        for rix in range(len(self.matrices["weights"])):
            Wij = self.matrices["weights"][rix] # extract the matrix wij
            #Ai = self.matrices["forward"][rix]
            delta_Oi = self.matrices["backward"][L-rix]
            for rows in range(len(Wij)):
                Zj = self.matrices["forward"][rix][rows]
                for col in range(len(Wij[rows,:])):
                    # w(n+1) = w(n) + alpha*w(n) + eat*Zj*delta
                    firstTerm = alpha*Wij[rows][col]
                    secondTerm = eta*Zj*delta_Oi[col]
                    new_Wij = Wij[rows][col] + firstTerm + secondTerm  
                    self.matrices["weights"][rix][rows][col] = new_Wij
                     

               


   